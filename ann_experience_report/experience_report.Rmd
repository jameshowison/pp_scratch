---
title: "PP Participation Project: Experience Report"
output: html_document
bibliography: cite.bib
---

Participating in the Smithsonian Digital Volunteers involved transcribing the text from various scanned documents into a text box, which makes these documents more accessible and searchable. Besides providing the transcriptions themselves, I also reviewed transcriptions from other volunteers, double checking that the transcribed text matched the text found on the document. All of the work was done on the Smithsonian Digital Volunteers' website. 
The experience I had while participating in the project was mostly in-line with what my expectations were. I expected to be frustrated with some of the transcriptions due to legibility, and that was definitely true. However, as I did more transcriptions, I became quicker at deciding whether I should spend the time on a particular document or not. This self-selection of tasks based on skill that I experienced is noted by Benkler as "an essential feature to commons-based peer production" [@benkler2006commons]. I found that volunteers would fill out a transcription to the best of their ability, and then they would leave it for others who would fill in the last missing piece. This is similar to developer interdependence, as described by @lindberg2016coordinating, although there is no cycle as contributors are not notified if their work is edited.The website does allow volunteers to leave comments about areas in their transcription that are unclear, but many of the half-completed transcriptions I would click on would have no communication about what aspect of the transcription was missing. This lack of communication is a big contrast to a different peer production community in which I monitored the communications of the participants, Wildwatch Burrowing Owl. The burrowing owl community has an entire section dedicated to answering clarifying questions about the owls that are being identified. The burrowing owl community likely needs much more communication in between participants to function since the task is more ambiguous and requires expert knowledge. Transcription, however, is a straightforward task and resolving issues within the transcription itself doesn't require much coordination or communication. 

While I was expecting to be frustrated with the transcription itself, I was not expecting to be frustrated by logistical issues. The website is surprisingly low-tech and bare bones, relying extremely heavily on humans, which led to quite a bit of squinting. For instance, many transcriptions are handwritten, and often the contrast between the ink and paper is low. Including image manipulation features within the site itself would have greatly improved my experience and facilitated the transcription of documents that were not high resolution. Additionally, as more projects were uploaded that were purely print documents, the lack of any computer-assisted transcription like Transkribus, first introduced by @kahle2017transkribus, actually made me less motivated to work on documents. Obviously, I could pump up my transcription numbers by transcribing these printed documents, but many were as simple as directly reading them off the page, which left me feeling like it wasn't the most effective use of my time. My favorite documents to transcribe were ones that required some concentration and fiddling but resulted in a sense of accomplishment.

## References
